{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\" align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.891 · Aprendizaje automático · PEC1</p>\n",
    "<p style=\"margin: 0; text-align:right;\">2024-2 · Máster universitario en Ciencia de datos (Data science)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "\n",
    "**PEC 1: Preparación de datos**\n",
    "\n",
    "El objetivo principal de esta primera PEC es que os familiaricéis con el entorno de trabajo que vais a utilizar en el resto de prácticas de la asignatura. Dicho entorno estará formado por un conjunto de dependencias a los módulos de Python necesarios para poder ejecutar de forma correcta el código que resuelve vuestra PEC. Estas dependencias las gestionaremos gracias a la ayuda de Anaconda, que, entre otras cosas, nos provee de un gestor de entornos virtuales para Python.\n",
    "\n",
    "Otra de las herramientas fundamentales del que será vuestro nuevo entorno de trabajo será Jupyter, que os permitirá trabajar con Notebooks (ficheros \\*.ipynb), como el presente enunciado, y donde podréis ejecutar vuestro código celda a celda, mostrando los resultados intermedios que necesitéis para comprender correctamente qué es lo que estáis haciendo en cada momento.\n",
    "\n",
    "En esta primera PEC, otro de los aspectos imprescindibles que vamos a cubrir, tal y como adelanta su título, es el de la preparación de los datos. En esta PEC aprenderemos a cargar distintos conjuntos de datos o _datasets_, los combinaremos y nos ayudaremos de herramientas de visualización para comprender mejor cómo se distribuye el dato, con el objetivo de entender cómo podemos sacarle partido. Además, nos habituaremos a trabajar con conjuntos de entrenamiento y test para confirmar si las conclusiones que sacamos sobre una parte de las muestras se pueden generalizar y extrapolar al resto.\n",
    "\n",
    "En resumen, en esta práctica veremos cómo aplicar diferentes técnicas para la carga y preparación de datos siguiendo los pasos listados a continuación:\n",
    "1. Carga y combinación de los distintos conjuntos de datos (2 puntos)<br>\n",
    "   1.1. Real Estate Valuation<br>\n",
    "   1.2. Taiwan Points of Interest (OpenStreetMap Export)<br>\n",
    "2. Análisis de los datos (2.5 puntos)<br>\n",
    "   2.1. Análisis estadístico básico<br>\n",
    "   2.2. Análisis exploratorio de los datos<br>\n",
    "3. Preprocesado de los datos (1 punto)<br>\n",
    "4. Reducción de la dimensionalidad (2 puntos)<br>\n",
    "5. Conjuntos desbalanceados de datos (1.5 puntos)<br>\n",
    "6. Búsqueda y combinación de nuevos conjuntos de datos (1 punto)<br>\n",
    "\n",
    "**Importante:** cada uno de los ejercicios puede suponer varios minutos de ejecución, por lo que la entrega debe hacerse en formato notebook y en formato html, donde se vea el código, los resultados y comentarios de cada ejercicio. Se puede exportar el notebook a html desde el menú File $\\to$ Download as $\\to$ HTML.\n",
    "\n",
    "**Importante:** existe un tipo de celda especial para albergar texto. Este tipo de celda os será muy útil para responder a las diferentes preguntas teóricas planteadas a lo largo de cada PEC. Para cambiar el tipo de celda a este tipo, elegid en el menú: Cell $\\to$ Cell Type $\\to$ Markdown.\n",
    "\n",
    "**Importante:** la solución planteada no debe utilizar métodos, funciones o parámetros declarados \"deprecated\" en futuras versiones.\n",
    "\n",
    "**Importante:** es conveniente que utilicéis una semilla con un valor fijo (en este Notebook se os propone la variable _seed_ inicializada a 100) en todos aquellos métodos o funciones que contengan alguna componente aleatoria para aseguraros de que obtendréis siempre el mismo resultado en las distintas ejecuciones de vuestro código.\n",
    "\n",
    "**Importante:** no olvidéis poner vuestro nombre y apellidos en la siguiente celda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Nombre y apellidos: Pablo Perez Verdugo</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la realización de la práctica necesitaremos installar e importar los siguientes módulos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install smogn==0.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import BallTree\n",
    "import smogn\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import folium\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "seed = 100\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Carga y combinación de los distintos conjuntos de datos (2 puntos)\n",
    "\n",
    "En esta PEC trabajaremos con un conjunto de datos en el que cada muestra representará la venta de un inmueble de la ciudad de Nueva Taipéi (Taiwán). Este conjunto de datos contiene una serie de columnas que nos dará información de la transacción y de algunas características de la vivienda. Sin embargo, queremos enriquecer dicho conjunto de datos añadiéndole un nuevo atributo descriptivo y esto lo haremos utilizando un segundo _dataset_, en el que cada muestra será un punto de interés de Taiwán, para calcular cuántos lugares de interés hay próximos a cada propiedad.\n",
    "\n",
    "Comencemos con la carga de datos:\n",
    "\n",
    "## 1.1. Real Estate Valuation\n",
    "\n",
    "El primer conjunto de datos, que además es el principal, se llama _Real Estate Valuation_ y es uno de los _datasets_ disponibles dentro del [Repositorio de Apdendizaje automático de la Universidad de California en Irvine](https://archive.ics.uci.edu/).\n",
    "\n",
    "En el enlace <https://archive.ics.uci.edu/dataset/477/real+estate+valuation+data+set> tenéis disponible tanto el mencionado _dataset_ _Real Estate Valuation_ como toda la información relevante para comprender mejor con qué tipo de dato vamos a trabajar. Tal y como se ha avanzado, las muestras de este conjunto de datos están relacionadas con ventas de inmuebles en la ciudad de Nueva Taipéi (Taiwán).\n",
    "\n",
    "En primer lugar, deberéis cargar en el Notebook el conjunto de datos con el que trabajaremos durante el resto de la PEC. Para ello podéis descargarlo manualmente del enlace referido previamente, aunque os aconsejamos que instaléis y utilicéis el módulo _ucimlrepo_ tal y como se explica en la propia página del _dataset_.\n",
    "\n",
    "**Nota:** junto al enunciado de la PEC hemos adjuntado el fichero *real+estate+valuation+data+set.zip* que podéis utilizar en el caso de que la página citada previamente no responda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> cargad el conjunto de datos \"Real Estate Valuation\" y mostrad:\n",
    "\n",
    "<ul>\n",
    "  <li>El número y nombre de los atributos descriptivos (variables que podrían ser usadas para predecir la variable objetivo \"y\").</li>\n",
    "  <li>El número de filas (muestras) del conjunto de datos.</li>\n",
    "  <li>Verificad si hay o no \"missing values\" y en qué columnas.</li>\n",
    "</ul>\n",
    "\n",
    "Sugerencia: si usáis ucimlrepo, explorad los atributos _metadata_ y _variables_ del objeto obtenido.\n",
    "    \n",
    "Sugerencia: separad el conjunto de datos original en las variables \"X\" (atributos descriptivos) e \"y\" (variable objetivo), aunque quizá pueda serte de utilidad en algún punto tenerlos también en un único DataFrame combinados.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "real_estate_valuation = fetch_ucirepo(id=477) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = real_estate_valuation.data.features \n",
    "y = real_estate_valuation.data.targets \n",
    "  \n",
    "# # metadata \n",
    "# print(real_estate_valuation.metadata) \n",
    "  \n",
    "# # variable information \n",
    "# print(real_estate_valuation.variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nuestro dataset tiene {X.shape[0]} filas y {X.shape[1]} columnas\")\n",
    "print(f\"El nombre de los atributos descriptivos son: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar ambos df para facilitarme los posteriores analisis\n",
    "X_y = pd.concat([X, y], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y.isnull().sum() # Ninguna columna tiene missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Pregunta:</strong> ¿el conjunto de datos propuesto es un problema de aprendizaje automático supervisado o no?, en el caso de serlo, ¿de qué tipo de aprendizaje supervisado estaríamos hablando?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sí, es un problema de aprendizaje automático supervisado ya que tenemos una variable que queremos predecir. Concretamente estamos ante un problema de regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Taiwan Points of Interest (OpenStreetMap Export)\n",
    "\n",
    "El segundo conjunto de datos, _Taiwan Points of Interest (OpenStreetMap Export)_, complementa al conjunto de datos inicial con la información de los puntos de interés presentes en Taiwán.\n",
    "\n",
    "La combinación de ambos conjuntos de datos la haremos mediante sus coordenadas geográficas. En nuestro caso, partiendo de la premisa de que lugares con un mayor número de puntos de interés cercanos podrían ser más caros, nos interesa calcular cuántos puntos de interés existen próximos a cada uno de los inmuebles que se han vendido.\n",
    "\n",
    "La búsqueda de los puntos de interés más cercanos a un inmueble puede hacerse por medio de fuerza bruta, recorriendo todos los puntos de interés para cada uno de los inmuebles. Sin embargo, este método no es precisamente el más eficiente, por lo que para el desarrollo de esta PEC os proponemos la utilización de la estructura de datos [BallTree](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html). Este tipo de árbol es una estructura de datos de partición espacial utilizada para organizar puntos en un espacio multidimensional. La estructura _BallTree_ divide los puntos del conjunto de datos en un conjunto anidado de bolas, tal y como se puede observar en el siguiente artículo: <https://medium.com/@geethasreemattaparthi/ball-tree-and-kd-tree-algorithms-a03cdc9f0af9>.\n",
    "\n",
    "El conjunto de datos se ha obtenido del siguiente enlace: <https://data.humdata.org/dataset/hotosm_twn_points_of_interest>, y os lo facilitamos junto al enunciado en el fichero *hotosm_twn_points_of_interest_points_geojson.geojson*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> calcula el número de puntos de interés a menos de 100 metros de distancia de cada inmueble y añádelo como un  atributo descriptivo más:\n",
    "\n",
    "<ul>\n",
    "  <li>Cargad el conjunto de datos \"Taiwan Points of Interest (OpenStreetMap Export)\" utilizando el método <i>read_file</i> de <i>geopandas</i>.</li>\n",
    "  <li>Inicializad una estructura de datos de tipo <i>BallTree</i> con los puntos de interés para posteriormente hacer búsquedas en ella.</li>\n",
    "  <li>Calculad el número de puntos de interés próximos a cada inmueble del primer conjunto de datos para añadírselo después como una nueva columna.</li>\n",
    "</ul>\n",
    "\n",
    "Nota: la latituda y longitud de cada lugar de interés viene almacenada en la columna \"geometry\" del conjunto de datos en una estructura llamada POINT, formada por los campos \"y\" (latitud) y \"x\" (longitud).\n",
    "\n",
    "Nota: dado que la tierra es más esférica que plana, es conveniente utilizar la métrica de distancia \"<a href=\"https://es.wikipedia.org/wiki/F%C3%B3rmula_del_semiverseno\">haversine</a>\" para el BallTree además de trabajar con radianes.\n",
    "\n",
    "Pista: para algunos cálculos será necesario utilizar el radio de la Tierra en metros.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos nuestro dataset con POIs de Taiwan\n",
    "gdf_twn_pois = gpd.read_file(\"hotosm_twn_points_of_interest_points_geojson-1.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un mapa en folium para visualizar mejor las tiendas y los POIs\n",
    "taiwan_map = folium.Map(location=[23.6978, 120.9605], zoom_start=8)\n",
    "\n",
    "# Cambiamos el fondo a CartoDB Positron para mayor claridad\n",
    "folium.TileLayer('CartoDB positron').add_to(taiwan_map)\n",
    "\n",
    "poi_sample = gdf_twn_pois.sample(5000, random_state=seed)\n",
    "for _, poi in poi_sample.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[poi.geometry.y, poi.geometry.x],  \n",
    "        radius=2,\n",
    "        color='red',\n",
    "        fill=True,\n",
    "        fill_color='red',\n",
    "        fill_opacity=0.5\n",
    "    ).add_to(taiwan_map)\n",
    "\n",
    "for _, house in X.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[house['X5 latitude'], house['X6 longitude']],  \n",
    "        popup=f\"House: {house['X5 latitude']}, {house['X6 longitude']}\",\n",
    "        icon=folium.Icon(color='blue', icon='home')\n",
    "    ).add_to(taiwan_map)\n",
    "\n",
    "taiwan_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos las coordenadas de la columna geometry\n",
    "coords = np.array([[geom.y, geom.x] for geom in gdf_twn_pois.geometry])\n",
    "\n",
    "# Creamos el BallTree [1]\n",
    "ball_tree = BallTree(np.radians(coords), metric='haversine')\n",
    "\n",
    "# Extraemos las coordenadas de las casas en un array numpy convertido a radianes\n",
    "house_coords = np.radians(X_y[['X5 latitude', 'X6 longitude']].to_numpy())\n",
    "\n",
    "# Definimos el radio en m\n",
    "radius_m = 100                   # Consideramos proximos los POIs a menos de 100 m\n",
    "radius_rad = radius_m / 6371000  # Convertimos m a radianes\n",
    "\n",
    "# Usamos el metodo query_radius para contar los POIs dentro del radio\n",
    "X_y['X7 num_pois_cercanos'] = ball_tree.query_radius(house_coords, r=radius_rad, count_only=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#ref1\">Bibliografia [1]</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Análisis de los datos (2.5 puntos)\n",
    "\n",
    "En este apartado visualizaremos cada una de las columnas o _features_ del dato para comprender mejor qué distribución tiene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Análisis estadístico básico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> realizad un análisis estadístico básico: \n",
    "<ul>\n",
    "  <li>Calculad estadísticos descriptivos básicos: media, mediana, desviación estandard, ...</li>\n",
    "  <li>Haced un histograma para cada variable.</li>\n",
    "</ul>\n",
    "Sugerencia: podéis usar la librería \"pandas\" y sus funciones \"describe\" y \"value_counts\", así como las funciones \"bar\", \"hist\" y \"hist2d\" de <i>matplotlib</i> (esta última os vendrá bien para mostrar un mapa de calor combinando las latitudes con las longitudes).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Las medianas son: \\n{X_y.median()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluimos las columnas de latitud y longitud ya que su histograma no nos aporta nada. Estas variables es mejor visualizarlas en el mapa que plotee previamente.\n",
    "columns = ['X1 transaction date', 'X2 house age', 'X3 distance to the nearest MRT station',\n",
    "           'X4 number of convenience stores','X7 num_pois_cercanos', 'Y house price of unit area']\n",
    "# Creamos histogramas para cada columna\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i, col in enumerate(columns):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.histplot(X_y[col], bins=25)\n",
    "    plt.title(f'Histogram of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voy a plotear este heatmap porque lo pide el enunciado, pero en mi opinion es mucho mejor visualizar la distribucion de lat/long con un mapa de folium\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist2d(X_y['X6 longitude'], X_y['X5 latitude'], bins=(25, 25), cmap='coolwarm')\n",
    "plt.colorbar(label='Count')\n",
    "plt.xlabel('Longitud')\n",
    "plt.ylabel('Latitud')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Análisis:</strong> comentad los resultados.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De los histogramas ploteados y las metricas obtenidas podemos extraer las siguientes conclusiones:\n",
    " - No tenemos informacion de ventas de todos los meses -> seguramente el dataset que nos han facilitado es un sample de otro mas completo\n",
    " - La casa mas antigua vendida tenia casi 40 años, siendo la media de 17.7 años\n",
    " - La mitad de las casas vendidas tenian una stacion MRT a menos de 500 m\n",
    " - No todas las casas tienen una convenience store cerca\n",
    " - Vemos que la cantidad de POIs cercanos es muy variable y no sigue una distribucion normal\n",
    " - El precio adimensionalizado de las casas sigue una distribucion normal a excepcion de unos outliers con precio superior a 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Análisis exploratorio de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este subapartado exploraremos gráficamente la relación de los atributos descriptivos con la variable objetivo y analizaremos las diferentes correlaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> usando una librería gráfica, como por ejemplo <i>matplotlib</i>, para cada una de las variables descriptivas, mostrad la distribución de ésta respecto a la variable objetivo, poniendo, por ejemplo, en el eje X los valores de la variable descriptiva y en el eje Y los de la objetivo.\n",
    "    \n",
    "La finalidad de este ejercicio es la de identificar de manera visual y rápida si algunos atributos nos permiten predecir mejor que otros el valor de la variable objetivo.\n",
    "\n",
    "Sugerencia: se recomienda utilizar <i>regplot</i>, de la biblioteca <i>seaborn</i>, ya que además de la distribución de puntos que se os pide os mostrará si hay alguna tendencia lineal positiva o negativa calculando una regresión lineal en cada caso.\n",
    "\n",
    "Sugerencia: de manera adicional, es conveniente ver la distribución geoespacial de los precios de las viviendas, por lo que se recomienda realizar un scatter plot con una escala de colores adecuada relacionada con el precio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No analizaremos latitud y longitud para predecir el precio de una vivienda porque la relación entre ubicación y precio es más espacial que lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_descriptivas = ['X1 transaction date', 'X2 house age', 'X3 distance to the nearest MRT station',\n",
    "                    'X4 number of convenience stores', 'X7 num_pois_cercanos']\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i, col in enumerate(variables_descriptivas):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.regplot(x=X_y[col], y=X_y['Y house price of unit area'], scatter_kws={'alpha':0.4}, line_kws={'color': 'red'})\n",
    "    plt.title(f'{col} vs Y house price of unit area')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sc = plt.scatter(X_y['X6 longitude'], X_y['X5 latitude'], c=X_y['Y house price of unit area'], cmap='coolwarm', alpha=0.7)\n",
    "plt.colorbar(sc, label='Precio de vivienda por unidad de area')\n",
    "plt.xlabel('Longitud')\n",
    "plt.ylabel('Latitud')\n",
    "plt.title('Distribución geoespacial de los precios de las viviendas')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Análisis:</strong>\n",
    "<br> Mirando las gráficas, ¿qué atributos parecen tener mayor influencia en el valor final de la variable objetivo? ¿Crees que con estos atributos sería suficiente para poder determinar el precio del inmueble?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los atributos con mayor influencia en el valor final de la variable objetivo son 'X3 distance to the nearest MRT station', 'X4 number of convenience stores' y 'X7 num_pois_cercanos' y creo que si son suficientes. Otra medida importante es el tamaño de la casa pero en nuestro caso no nos afecta ya que la variable a predecir que es el precio de venta de la casa esta expresada en 3030 $New Taiwan Dollar/m^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> identifica y elimina los valores atípicos del conjunto de datos y repite las gráficas realizadas en el ejercicio anterior para analizar cómo podrían estar afectandonos este tipo de muestras.\n",
    "\n",
    "Es muy importante que este paso lo realicéis en una copia del dato para no perder los valores atípicos del conjunto con el que estamos trabajando, ya que continuaremos trabajando con todas las muestras del dataset original en los siguientes apartados.\n",
    "\n",
    "Nota: puedes utilizar, por ejemplo, el método IQR (Interquartile Range).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una copia, para no perder nuestro df original\n",
    "X_y_no_outliers = X_y.copy()\n",
    "\n",
    "# Creamos una funcion para eliminar outliers basandonos en el IQR\n",
    "def eliminar_outliers_iqr(df, columns):\n",
    "    Q1 = df[columns].quantile(0.25)\n",
    "    Q3 = df[columns].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    min_threshold = Q1 - 1.5 * IQR\n",
    "    max_threshold = Q3 + 1.5 * IQR\n",
    "\n",
    "    return df[~((df[columns] < min_threshold) | (df[columns] > max_threshold)).any(axis=1)]\n",
    "\n",
    "X_y_no_outliers = eliminar_outliers_iqr(X_y_no_outliers, variables_descriptivas + ['Y house price of unit area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repetimos nuestros plots\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i, col in enumerate(variables_descriptivas):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.regplot(x=X_y_no_outliers[col], y=X_y_no_outliers['Y house price of unit area'], scatter_kws={'alpha':0.4}, line_kws={'color': 'red'})\n",
    "    plt.title(f'{col} vs Y house price of unit area')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sc = plt.scatter(X_y_no_outliers['X6 longitude'], X_y_no_outliers['X5 latitude'], c=X_y_no_outliers['Y house price of unit area'], cmap='coolwarm', alpha=0.7)\n",
    "plt.colorbar(sc, label='Precio de vivienda por unidad de area')\n",
    "plt.xlabel('Longitud')\n",
    "plt.ylabel('Latitud')\n",
    "plt.title('Distribución geoespacial de los precios de las viviendas')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Análisis:</strong> explica qué ocurre.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ha cambiado el eje y ya que al eliminar outliers ya no va desde 0 a 120 sino que ahora va desde 0 hasta 70. Respecto a la tendencia de las lineas de regresion no ha cambiado ya que estos outliers no afectaban mucho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> calculad y mostrad la correlación entre las variables numéricas.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para ello ploteamos la matrix de correlacion\n",
    "corr_matrix = X_y_no_outliers.drop(columns = [\"X5 latitude\", \"X6 longitude\"]).corr()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Matrix de correlacion\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Análisis:</strong> comenta los resultados.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que como comente antes los atributos con mayor influencia en el precio de la vivienda son 'X3 distance to the nearest MRT station', 'X4 number of convenience stores' y 'X7 num_pois_cercanos'. Aunque observamos un problema y es que hay multicolinearidad entre estas 3 variables lo cual viola uno de los principios de la regresión lineal. Ademas vemos que la transacion date no tiene relacion con el precio de la vivienda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocesado de los datos (1 punto)\n",
    "\n",
    "Una vez analizados los atributos descriptivos, es el momento de prepararlos para que nos sean útiles de cara a predecir valores.\n",
    "\n",
    "En este apartado:\n",
    "<li>Estandarizaremos los valores de los atributos descriptivos numéricos para que sus escalas no sean muy diferentes.</li>\n",
    "<li>Separaremos el conjunto de datos original en dos subconjuntos: entrenamiento y test.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> estandariza todos los atributos descriptivos numéricos, este será el nuevo conjunto de atributos descriptivos con el que trabajaremos desde ahora.\n",
    "<hr>\n",
    "Sugerencia: utilizad \"StandardScaler\" de <i>preprocessing</i>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler() # [2]\n",
    "# Eliminamos lat/long ya que la relacion que tienen con Y en espacial no lineal\n",
    "X_y_standarizado = X_y.drop(columns = [\"X5 latitude\", \"X6 longitude\"]).copy()\n",
    "X_y_standarizado[variables_descriptivas] = scaler.fit_transform(X_y_standarizado[variables_descriptivas])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#ref2\">Bibliografia [2]</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> separa los atributos descriptivos escalados y la variable objetivo en los subconjuntos de entrenamiento y test.\n",
    "<hr>\n",
    "Sugerencia: para separar entre train y test podéis usar \"train_test_split\" de sklearn.<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_y_standarizado[variables_descriptivas]\n",
    "y = X_y_standarizado['Y house price of unit area']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Análisis:</strong> explica si la decisión de transformar el conjunto de datos (estandarización) antes de realizar la separación del conjunto de datos en los subconjuntos de entrenamiento y test es una buena idea.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, no es una buena idea ya que de esa forma la media y la desviacion estandar calculadas para el conjunto entero habran participado en la estandarizacion de los datos de test. Esto no es bueno ya que introducimos un bias en los datos de test y lo que queremos es que sean lo mas independientes posible. Lo correcto hubiera sido dividir entre train y test y luego estandarizamos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Análisis:</strong> en este ejercicio hemos estandarizado los valores de los atributos descriptivos para que sus escalas no sean muy diferentes. ¿Qué nos aporta estandarizar los atributos descriptivos? ¿hay alguna situación o escenario en la que sea imprescindible?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estandarizar los atributos descriptivos es muy importante cuando las escalas son muy diferentes ya que el modelo puede dar mas importancia sino a la variable con valores mas grandes.\n",
    "Ademas la estandarizacion mejora la convergencia de los modelos. Por ejemplo en mi trabajo resuelvo muchos problemas de MILP usando gurobi y vemos un notable cambio en computing time de cuando las variables estan estandarizadas respecto a cuando no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Reducción de la dimensionalidad (2 puntos)\n",
    "\n",
    "En este apartado retomaremos el análisis gráfico de distribución de la variable objetivo a lo largo de las muestras del conjunto de datos. En el segundo apartado pudimos observar si las variables descriptivas por separado eran muy prometedoras o no de cara a la predicción. Aquí vamos a intentar determinar si su combinación puede ayudarnos a determinar el precio del inmueble de mejor manera que utilizando los atributos por separado. Con este propósito, vamos a reducir la dimensionalidad del problema a solamente dos atributos, que serán la proyección de los atributos descriptivos originales, y observaremos de qué manera se distribuyen las muestras en función de su valor objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong><br>\n",
    "<ul>\n",
    "    <li>Aplicad el método de reducción de la dimensionalidad Principal Component Analysis (PCA) para reducir a 2 dimensiones el dataset entero con todas las features.</li>\n",
    "    <li>Con el objetivo de visualizar si es posible predecir eficientemente el valor de la variable objetivo con este método, generad un gráfico en 2D con el resultado del PCA utilizando una escala de colores que permita distinguir de forma sencilla si los valores altos o bajos se acumulan más o menos en determinadas zonas.</li>\n",
    "</ul>\n",
    "    \n",
    "NOTA: Tened cuidado, no incluyáis la variable objetivo en la reducción de dimensionalidad. Queremos explicar la variable objetivo en función del resto de variables reducidas a dos dimensiones.\n",
    "\n",
    "<hr>\n",
    "Sugerencia: no es necesario que programéis el algoritmo de PCA, podéis usar la implementación disponible en la librería de <i>scikit-learn</i>.<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = X_y_standarizado[variables_descriptivas]\n",
    "\n",
    "# Aplicamos PCA para reducir la dimensionalidad\n",
    "pca = PCA(n_components=2)\n",
    "X_pca_transformed = pca.fit_transform(X_pca)\n",
    "X_pca_df = pd.DataFrame(X_pca_transformed, columns=['PCA1', 'PCA2'], index=X_pca.index)\n",
    "\n",
    "# Añadimos el precio de la vivienda para poder visualizarlo\n",
    "X_pca_df['Y house price of unit area'] = X_y_standarizado['Y house price of unit area']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#ref3\">Bibliografia [3]</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sc = plt.scatter(X_pca_df['PCA1'], X_pca_df['PCA2'], c=X_pca_df['Y house price of unit area'], cmap='coolwarm', alpha=0.7)\n",
    "plt.colorbar(sc, label='Precio de la vivienda por unidad de area')\n",
    "plt.xlabel('PCA1')\n",
    "plt.ylabel('PCA2')\n",
    "plt.title('PCA proyeccion 2D')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong><br>\n",
    "<ul>\n",
    "    <li>Repetid la reducción de dimensionalidad, pero en este caso usando TSNE.  Podéis encontrar más información acerca de este algoritmo en el siguiente enlace: <a href=\"https://distill.pub/2016/misread-tsne\">https://distill.pub/2016/misread-tsne/</a></li>\n",
    "    <li>Al igual que antes, generad un gráfico en 2D con el resultado del TSNE utilizando una escala degradada de colores para la variable objetivo (\"y\"), con el fin de visualizar si es posible predecir eficientemente el valor de la variable objetivo con este método.</li>\n",
    "</ul>\n",
    "\n",
    "<hr>\n",
    "Sugerencia: no es necesario que programéis el algoritmo TSNE, podéis usar la implementación disponible en la librería de <i>scikit-learn</i>.<br>\n",
    "Sugerencia: a parte de especificar el número de componentes, probad a usar los parámetros \"learning_rate\" y \"perplexity\".<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne = X_y_standarizado[variables_descriptivas].copy()\n",
    "\n",
    "# Aplicamos TSNE para reducir la dimensionalidad [4]\n",
    "tsne = TSNE(n_components=2, learning_rate=251, perplexity=25, random_state=seed)\n",
    "X_tsne_transformed = tsne.fit_transform(X_tsne)\n",
    "X_tsne_df = pd.DataFrame(X_tsne_transformed, columns=['TSNE1', 'TSNE2'], index=X_y_standarizado.index)\n",
    "\n",
    "# Añadimos el precio de la vivienda para poder visualizarlo\n",
    "X_tsne_df['Y house price of unit area'] = X_y_standarizado['Y house price of unit area']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#ref4\">Bibliografia [4]</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sc = plt.scatter(X_tsne_df['TSNE1'], X_tsne_df['TSNE2'], c=X_tsne_df['Y house price of unit area'], cmap='coolwarm', alpha=0.7)\n",
    "plt.colorbar(sc, label='Precio de la vivienda por unidad de area')\n",
    "plt.xlabel('TSNE1')\n",
    "plt.ylabel('TSNE2')\n",
    "plt.title('TSNE proyeccion 2D')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Análisis:</strong> observando los dos gráficos, ¿crees que ha funcionado bien la reducción de dimensionalidad? ¿Crees que será útil para predecir el valor de la variable objetivo? ¿Cuál de los dos métodos ha funcionado mejor? ¿Por qué obtenemos resultados tan diferentes?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En mi opinion no han conseguido reducir bien la dimensionalidad ya que ambos scatter plots son dispersos no se aprecian ningunas agrupaciones definidas. Creo que no sera util para perdecir el valor de la variable objetivo. Si tuviera que quedarme con uno seria con el TSNE ya que consigue agrupar los puntos un poco mas. Obtenemos resultados tan diferentes porque PCA es linea mientras que TSNE no, ademas de que TSNE tiene varios parametros con los que hemos ido jugando obteniendo graficos cada vez distintos. Un leve cambio en el learning rate o preplexity cambia la gráfica entera.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Pregunta:</strong> ¿qué opinas de TSNE como opción para reducir la dimensionalidad? ¿Qué te parece que sólo tenga el método \"fit_transform\" pero no tenga \"transform\"? ¿Conoces alguna otra opción que, con unas prestaciones similares, evite los problemas que tiene TSNE?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TSNE esta bien para visualizar estructuras no lineales en datos con muchas dimensiones, pero no es adecuado para reducir la dimensionalidad porque no permite transformar nuevos datos (no tiene metodo transform) y es computacionalmente costoso. Como alternativa UMAP [5] ofrece prestaciones similares, siendo más rápido y permite transformar nuevos datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#ref5\">Bibliografia [5]</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conjuntos desbalanceados de datos (1.5 puntos)\n",
    "\n",
    "En los problemas con variables objetivo discretas, con valores acotados, es muy común encontrar conjuntos de datos muy desbalanceados. En la industria existen múltiples ejemplos, como la detección de fraude o la fuga de clientes. Sin embargo, este inconveniente también está presente en problemas con variable objetivo continua, donde puede apreciarse que la distribución de dicha variable se aleja mucho de ser uniforme.\n",
    "\n",
    "El caso del _dataset_ con el que estamos trabajando, tal y como pudiste observar en el apartado de análisis de los datos, no presenta una distribución uniforme, por lo que no existen tantas muestras de inmuebles caros y/o muy baratos en comparación con el número de viviendas de precios intermedios. Este hecho puede llevar a algunos algoritmos de aprendizaje a priorizar la predicción de los valores más comunes, marginando aquellos que no son tan habituales. Con el fin de mitigar esta problemática, existen diferentes algoritmos de balanceo que optan por la eliminación de las muestras más comunes (_undersampling_) o, por el contrario, tratan de generar nuevas muestras para los valores más escasos de la variable objetivo (_oversampling_).\n",
    "\n",
    "En este caso, os proponemos utilizar un algoritmo que aplica ambas ideas: _under_ y _oversampling_, redistribuyendo las muestras para que la variable objetivo presente una distribución más uniforme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> utiliza <i>smoter</i>, de la biblioteca <i>smogn</i>, para equilibrar la distribución de la variable objetivo. Una vez aplicado el algoritmo, analiza cómo ha quedado la distribución de la variable objetivo y el número de muestras totales que obtenemos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos smoter [6]\n",
    "X_y_balanceado = smogn.smoter(data=X_y, y='Y house price of unit area')\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(X_y['Y house price of unit area'], bins=25, color='blue')\n",
    "plt.title(\"Distribución Original de la variable objetivo\")\n",
    "plt.xlabel(\"Precio de vivienda por unidad de area\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(X_y_balanceado['Y house price of unit area'], bins=25, color='blue')\n",
    "plt.title(\"Distribución Balanceada de la variable objetivo\")\n",
    "plt.xlabel(\"Precio de vivienda por unidad de area\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Numero de samples despues el balanceo: {X_y_balanceado.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que ahora hay mas casos cercanos a 120, ademas de una distribución un poco mas uniforme.\n",
    "\n",
    "<a href=\"#ref6\">Bibliografia [6]</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el objetivo de comprender mejor y de manera visual cómo se generan estas nuevas muestras utilizaremos, a partir de ahora, la descomposición a dos dimensiones que mejor se haya comportado en el apartado anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> mostrad, mediante un scatter plot en función de las dos componentes a las que anteriormente hemos reducido el dataset, la distribución de los precios de venta del conjunto de datos original y el obtenido al aplicar <i>smoter</i>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos smoter\n",
    "X_tsne_df_balanceado = smogn.smoter(data=X_tsne_df, y='Y house price of unit area')\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sc = plt.scatter(X_tsne_df['TSNE1'], X_tsne_df['TSNE2'], c=X_tsne_df['Y house price of unit area'], cmap='coolwarm', alpha=0.7)\n",
    "plt.colorbar(sc, label='Precio de la vivienda por unidad de area')\n",
    "plt.xlabel('TSNE1')\n",
    "plt.ylabel('TSNE2')\n",
    "plt.title('TSNE proyeccion 2D')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sc = plt.scatter(X_tsne_df_balanceado['TSNE1'], X_tsne_df_balanceado['TSNE2'], c=X_tsne_df_balanceado['Y house price of unit area'], cmap='coolwarm', alpha=0.7)\n",
    "plt.colorbar(sc, label='Precio de la vivienda por unidad de area')\n",
    "plt.xlabel('TSNE1')\n",
    "plt.ylabel('TSNE2')\n",
    "plt.title('TSNE proyeccion 2D')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Análisis:</strong> comentad los resultados.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el punto rojo a la altura de TSNE1 = 20 que antes era un outlier ahora no es un caso aislado sino que hay mas puntos rojos a su alrededor. Por lo cual vemos que smoter ha cumplido su funcion y ha conseguido que una minoría de casos quede mas representada, para que asi a la hora de entrenar el modelo este se ajuste tambien a los casos minoritarios. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Búsqueda y combinación de nuevos conjuntos de datos (1 punto)\n",
    "\n",
    "En este apartado os animamos a que busquéis de manera libre un nuevo conjunto de datos que podáis combinar de alguna forma (a través de alguno o algunos atributos comunes) con el conjunto de datos inicial para enriquecer más aún la información que tenemos de cada vivienda de cara a estimar mejor su precio.\n",
    "\n",
    "Es inprescindible que, una vez tengáis el conjunto de datos ampliado con el cálculo de vuestros nuevos atributos descriptivos procedentes del nuevo _dataset_, reejecutéis todo el Notebook, a excepción de la carga inicial del conjunto de datos _Real Estate Valuation_, para comprobar el efecto de dicha ampliación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografía\n",
    "\n",
    "<a id=\"ref1\"></a> \n",
    "1. https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html\n",
    "<a id=\"ref2\"></a>\n",
    "2. https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "<a id=\"ref3\"></a>\n",
    "3. https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "<a id=\"ref4\"></a>\n",
    "4. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
    "<a id=\"ref5\"></a>\n",
    "5. https://umap-learn.readthedocs.io/en/latest/basic_usage.html\n",
    "<a id=\"ref6\"></a>\n",
    "6. https://pypi.org/project/smogn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pec1_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
